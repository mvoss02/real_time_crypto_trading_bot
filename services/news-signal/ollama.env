MODEL_NAME=llama3.2:3b
OLLAMA_BASE_URL=http://127.0.0.1:11434 

# Ollama has to be installed in order for it to work on the given device.
# Depending on the service which is being run, one needs different access points:
# http://ollama:11434 -- When run inside the ollama container
# http://127.0.0.1:11434 --  For local 
# http://host.docker.internal:11434 -- For mac (has to be installed on mac - no GPU support)